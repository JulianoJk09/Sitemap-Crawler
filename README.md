# Sitemap-Crawler
## Introduction
A sitemap crawler is a tool or program that is designed to automatically scan and crawl through the pages of a website, using the URLs provided in its sitemap file. The file will contain a list of URLs on a website that the e owner wants search engines to index. By using this, website owners can ensure that all the pages listed in their sitemap file are accessible and properly linked, and that there are no broken links or errors that could negatively affect their search engine rankings. Sitemap crawlers can be used for many different purposes, such as checking for broken links or duplicate content, identifying pages that are not properly linked, and ensuring that all pages are properly indexed by search engines. 

##Tldr
In this project we will develop an application that will have the ability to scan the link of website and search for broken links, corrupted photos, and videos.
